# tests/reporting Knowledge

## Purpose

The `tests/reporting/` directory contains tests for validating Firmo's report generation system. This includes testing the core reporting logic found in `lib/reporting/init.lua` and verifying the output of the various individual report formatters located under `lib/reporting/formatters/`. The goal is to ensure that test results, code coverage data, and quality metrics are correctly processed, formatted into standard outputs (like HTML, JSON, JUnit, LCOV, Cobertura, TAP, CSV, Summary), and saved to the filesystem according to configuration.

## Key Concepts

The tests in this directory cover the main aspects of the reporting system:

- **Core Reporting Logic (`core_test.lua`):** These tests likely validate the central orchestration functions in `lib/reporting/init.lua`. This involves:
    - Testing configuration handling (`configure`, interaction with `central_config`).
    - Verifying the loading and registration of built-in formatters (via `load_formatter_registry`).
    - Ensuring the correct formatter function is selected based on the format string passed to `format_coverage`, `format_quality`, or `format_results`.
    - Testing the main saving functions (`save_coverage_report`, `save_quality_report`, `save_results_report`) to ensure they call the appropriate formatter and then attempt to write the output using `lib.tools.filesystem` (often mocking the write operation).
    - Validating the `auto_save_reports` function's logic for generating multiple configured reports.
    - Testing calls to data validation helpers (if present in `lib.reporting.validation`).

- **Formatter Implementation (`formatter_test.lua`):** These tests focus on the correctness of the output generated by each individual formatter module (e.g., `html.lua`, `json.lua`, `junit.lua`). For every supported format:
    - **Input:** The test provides predefined sample input data (Lua tables representing mock test results, coverage stats, or quality metrics).
    - **Execution:** It calls the specific formatter function (e.g., `require("lib.reporting.formatters.junit").format_results(data, options)`).
    - **Validation:** It asserts that the returned string (or table for JSON) matches the expected structure and content for that specific report format. This validation might involve:
        - Simple string `match` or `find` for key elements.
        - Comparison against complete, known-good "golden" output strings or files.
        - Parsing the output (e.g., using an XML or JSON parser if available) to verify its structural integrity and data content.

## Usage Examples / Patterns (Illustrative Test Snippets)

### Testing Core Reporting Logic (Conceptual)

```lua
--[[
  Example test verifying save_coverage_report orchestration (mocking file write).
]]
local describe, it, expect = firmo.describe, firmo.it, firmo.expect
local reporting = require("lib.reporting")
local test_helper = require("lib.tools.test_helper")
-- Mock the internal write_file function to avoid actual disk I/O
local write_file_spy = firmo.spy.spy_on(reporting, "write_file").returns(true)

describe("Reporting Core", function()
  after_each(function()
    write_file_spy:restore()
  end)

  it("save_coverage_report should format data and call write_file", function()
    local mock_data = { summary = { total_lines=20, covered_lines=15 }, files = { ["a.lua"]={...} } }
    local target_path = "my_report.html"

    local success, err = reporting.save_coverage_report(target_path, mock_data, "html")

    expect(err).to_not.exist()
    expect(success).to.be_truthy()
    -- Verify write_file was called with the correct path and some HTML string content
    expect(write_file_spy).to.be.called_once()
    expect(write_file_spy.calls[1].args[1]).to.equal(target_path)
    expect(write_file_spy.calls[1].args[2]).to.be.a("string")
    expect(write_file_spy.calls[1].args[2]).to.match("<html") -- Check it looks like HTML
  end)
end)
```

### Testing a Formatter (Conceptual JUnit Example)

```lua
--[[
  Example test verifying the output of the JUnit formatter.
]]
local describe, it, expect = firmo.describe, firmo.it, firmo.expect
-- Assuming formatters can be accessed, e.g., via the main module or direct require
local reporting = require("lib.reporting")
-- Or potentially: local junit_formatter = require("lib.reporting.formatters.junit")

it("junit formatter should produce basic XML structure", function()
  local mock_results = {
    name = "MyTestSuite", tests = 2, failures = 1, errors = 0, skipped = 0, time = 1.23,
    test_cases = {
      { name = "TestA", classname = "Module1", time = 0.05, status = "pass" },
      { name = "TestB", classname = "Module1", time = 1.18, status = "fail",
        failure = { message = "Assertion failed", type = "FAILURE", details = "Expected true == false" } },
    }
  }

  -- Get the formatter function (method depends on how formatters are exposed)
  local formatter_fn = reporting.get_formatter("junit", "results")
  expect(formatter_fn).to.be.a("function")

  -- Call the formatter
  local output_string = formatter_fn(mock_results)

  -- Assert on the output string
  expect(output_string).to.be.a("string")
  expect(output_string).to.match("<testsuite name=\"MyTestSuite\" tests=\"2\" failures=\"1\"")
  expect(output_string).to.match("<testcase name=\"TestA\" classname=\"Module1\"")
  expect(output_string).to.match("<testcase name=\"TestB\" classname=\"Module1\"")
  expect(output_string).to.match("<failure message=\"Assertion failed\" type=\"FAILURE\">")
end)
```

**Note:** Examples using `firmo.generate_*` or `firmo.register_formatter` found in the previous version are **incorrect** based on the likely API structure and should be disregarded for understanding these tests.

## Related Components / Modules

- **Module Under Test:**
    - `lib/reporting/knowledge.md` (Core logic)
    - `lib/reporting/formatters/knowledge.md` (Formatter overview)
    - Individual formatter modules (e.g., `lib/reporting/formatters/html.lua`, `json.lua`, `junit.lua`, `lcov.lua`, `cobertura.lua`, `tap.lua`, `csv.lua`, `summary.lua`)
- **Test Files:**
    - `tests/reporting/core_test.lua`
    - `tests/reporting/formatter_test.lua`
- **Dependencies:**
    - `lib/core/central_config/knowledge.md`: For configuring report formats, paths, and options.
    - `lib/tools/filesystem/knowledge.md`: Used by `reporting.write_file` to save reports.
    - `lib/tools/error_handler/knowledge.md`: For handling errors during formatting and file I/O.
    - `lib/tools/json/knowledge.md`: Used by the JSON formatter and potentially `reporting.write_file`.
- **Data Sources:** Tests need input data representing output from:
    - `lib/coverage/knowledge.md`
    - `lib/quality/knowledge.md`
    - Test Runner (results like pass/fail counts, test names, durations, error messages).
- **Parent Overview:** `tests/knowledge.md`

## Best Practices / Critical Rules (Optional)

- **Representative Mock Data:** Use mock input data for formatters that covers various scenarios: passing tests, failing tests, skipped tests, errors, different coverage levels (full, partial, none), quality issues found/not found.
- **Rigorous Output Validation:**
    - For structured formats (XML, JSON): If possible, use a parser within the test to validate the output's structure and key data points. At minimum, use precise string matching for critical tags/attributes/keys.
    - For text-based formats (LCOV, TAP, CSV, Summary): Use string matching (`expect().to.match()`) for specific lines, tags, or values.
    - For HTML: Check for the presence of essential HTML tags (`<html>`, `<head>`, `<body>`), specific CSS classes, and key data points within the content. Avoid brittle tests relying on exact whitespace or complex layout.
    - Consider using "snapshot testing" or comparing against known-good "golden files" for complex formats, but be mindful that maintaining these golden files can be effort-intensive.
- **Test Formatter Options:** Ensure tests verify the behavior of configuration options specific to each formatter (e.g., `json.pretty`, `html.theme`, `summary.detailed`).
- **Isolate File I/O:** When testing the core reporting logic (`save_*` functions), consider mocking `reporting.write_file` or `lib.tools.filesystem.write_file` to isolate the test from actual disk operations and focus on verifying correct path generation and content formatting.

## Troubleshooting / Common Pitfalls (Optional)

- **Incorrect Report Content/Data:**
    - **Cause:** The formatter logic (`lib/reporting/formatters/*.lua`) might be misinterpreting the input data or have a bug in its transformation logic. The mock input data provided by the test might also be incorrect or incomplete.
    - **Debugging:** Examine the mock input data structure carefully. Add logging or step through the specific formatter function being tested to trace how it processes the data and generates the output.
- **Invalid Report Format (Syntactically Incorrect):**
    - **Cause:** The formatter logic is producing output that violates the rules of the target format (e.g., malformed XML/JSON, incorrect LCOV tags).
    - **Debugging:** Use external validation tools (XML/JSON linters/validators, LCOV parsers) on the output string generated by the failing test to pinpoint the syntax error. Correct the formatter logic.
- **File Saving Failures (`save_*_report`):**
    - **Cause:** Issues within `reporting.write_file` or the underlying `lib.tools.filesystem` module (e.g., invalid output path, lack of write permissions).
    - **Debugging:** Check the error object returned by the `save_*` function. Verify the target directory exists and has write permissions. Add logging to `reporting.write_file` or examine `filesystem` logs.
- **Configuration Issues:**
    - **Cause:** Formatter-specific options not taking effect, reports saving to unexpected locations, default format not being used correctly.
    - **Debugging:** Verify how configuration is loaded and accessed within `lib/reporting/init.lua` and the specific formatter. Check `central_config` values. Examine the path generation logic in `auto_save_reports` or `save_*` functions if paths are wrong.
